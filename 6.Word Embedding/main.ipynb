{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"The quick brown fox jumps over the lazy dog.\",\n",
    "         \"She sells seashells by the seashore.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max([len(seq) for seq in sequences])\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b28cbbfc10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(word_index) + 1, embedding_dim, input_length=max_sequence_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(data, np.array([1, 0]), epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0089439  -0.07898594  0.08235931 -0.01468321 -0.06246345  0.09323414\n",
      " -0.07902019 -0.07921599  0.07818613  0.02889836  0.01549651 -0.02560932\n",
      "  0.00426028 -0.06824932 -0.0653531   0.01835645  0.03950776  0.01032393\n",
      " -0.01991956  0.01061196  0.00740094 -0.06669379 -0.00773672  0.06884062\n",
      " -0.08262392 -0.07227772  0.01240869 -0.00702563 -0.02955525  0.0360644\n",
      "  0.01579091 -0.04592925 -0.03959807 -0.02812852 -0.04882526 -0.06076057\n",
      "  0.08645484  0.00249765  0.02856142  0.00735773  0.09971799  0.07866419\n",
      " -0.04757039 -0.09275425  0.05827431 -0.07744529 -0.05495697  0.03260503\n",
      "  0.08817302  0.01634433 -0.07647078  0.00350732  0.00903309 -0.00745112\n",
      " -0.06362172 -0.00728736 -0.02681381  0.08642831 -0.03175977 -0.01330984\n",
      "  0.06916974  0.0643432  -0.05530996  0.0054668   0.00637574  0.01776946\n",
      " -0.00126344  0.07122429  0.06785431 -0.00418438 -0.01513732 -0.07391652\n",
      " -0.01924435 -0.04614468 -0.02834777 -0.04963019 -0.03311981 -0.05551278\n",
      "  0.01279954  0.08186339 -0.08097879 -0.03151198 -0.09283044 -0.05525881\n",
      " -0.01221804  0.01284757 -0.0081212  -0.09109122 -0.02225615 -0.00104472\n",
      " -0.06392759 -0.00142868 -0.04672252 -0.07329866 -0.0413296   0.04342953\n",
      "  0.05410653 -0.00286529 -0.03197929 -0.03337475]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.get_weights()[0]\n",
    "vector = word_vectors[word_index['jumps']]\n",
    "print(vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
